{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOxbALYztmVMHLRp6KXfM3F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yeamuhid/Meching-Learning-/blob/main/Reinforcement_of_transfer_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZLONsLEGrs90",
        "outputId": "909dc64b-c6b9-4200-d939-d7f6beb4bb72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:55: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs; see https://jax.readthedocs.io/en/latest/aot.html. For example, replace xla_computation(f)(*xs) with jit(f).lower(*xs).compiler_ir('hlo'). See CHANGELOG.md for 0.4.30 for more examples.\n",
            "  from jax import xla_computation as _xla_computation\n"
          ]
        }
      ],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import load_model, Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Load pre-trained model for transfer\n",
        "def load_pretrained_model(filepath, new_output_dim):\n",
        "    # Load the pre-trained model\n",
        "    model = load_model(filepath)\n",
        "    # Remove the last layer and add a new output layer\n",
        "    model.pop()\n",
        "    model.add(Dense(new_output_dim))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import load_model, Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Load pre-trained model and adapt it to new task\n",
        "def create_model_from_pretrained(filepath, input_dim, output_dim):\n",
        "    try:\n",
        "        # Load the pre-trained model\n",
        "        pretrained_model = load_model(filepath)\n",
        "        # Remove the last layer (if dimensions don't match) and add a new output layer\n",
        "        model = Sequential(pretrained_model.layers[:-1])\n",
        "    except Exception as e:\n",
        "        print(\"Pretrained model could not be loaded. Creating a new one.\")\n",
        "        model = Sequential()\n",
        "        model.add(Dense(24, input_dim=input_dim, activation='relu'))\n",
        "        model.add(Dense(24, activation='relu'))\n",
        "    # Add output layer specific to the new task\n",
        "    model.add(Dense(output_dim, activation='linear'))\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
        "    return model\n",
        "\n",
        "# Simple Q-learning algorithm with transfer\n",
        "def train_rl_model(env_name, pretrained_filepath=None, episodes=500):\n",
        "    # Initialize the environment\n",
        "    env = gym.make(env_name)\n",
        "    state_size = env.observation_space.shape[0]\n",
        "    action_size = env.action_space.n\n",
        "\n",
        "    # Load or create the RL model\n",
        "    model = create_model_from_pretrained(pretrained_filepath, input_dim=state_size, output_dim=action_size)\n",
        "\n",
        "    # Hyperparameters\n",
        "    gamma = 0.95   # Discount factor\n",
        "    epsilon = 1.0  # Exploration rate\n",
        "    epsilon_decay = 0.995\n",
        "    epsilon_min = 0.01\n",
        "    batch_size = 32\n",
        "    replay_buffer = []\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        state = env.reset()\n",
        "        state = np.reshape(state, [1, state_size])\n",
        "        total_reward = 0\n",
        "\n",
        "        while True:\n",
        "            # Exploration vs Exploitation\n",
        "            if np.random.rand() <= epsilon:\n",
        "                action = np.random.choice(action_size)\n",
        "            else:\n",
        "                q_values = model.predict(state)\n",
        "                action = np.argmax(q_values[0])\n",
        "\n",
        "            # Take the action\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            next_state = np.reshape(next_state, [1, state_size])\n",
        "            total_reward += reward\n",
        "\n",
        "            # Store in replay buffer\n",
        "            replay_buffer.append((state, action, reward, next_state, done))\n",
        "            if len(replay_buffer) > 2000:  # Limit buffer size\n",
        "                replay_buffer.pop(0)\n",
        "\n",
        "            # Train the model using samples from the replay buffer\n",
        "            if len(replay_buffer) > batch_size:\n",
        "                minibatch = np.random.choice(replay_buffer, batch_size)\n",
        "                for s, a, r, s_next, d in minibatch:\n",
        "                    target = r\n",
        "                    if not d:\n",
        "                        target += gamma * np.amax(model.predict(s_next)[0])\n",
        "                    target_f = model.predict(s)\n",
        "                    target_f[0][a] = target\n",
        "                    model.fit(s, target_f, epochs=1, verbose=0)\n",
        "\n",
        "            state = next_state\n",
        "\n",
        "            if done:\n",
        "                print(f\"Episode {episode + 1}/{episodes}, Total Reward: {total_reward}\")\n",
        "                break\n",
        "\n",
        "        # Decay exploration rate\n",
        "        if epsilon > epsilon_min:\n",
        "            epsilon *= epsilon_decay\n",
        "\n",
        "    # Save the trained model\n",
        "    model.save(\"trained_rl_model.h5\")\n",
        "    return model\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    trained_model = train_rl_model(env_name=\"CartPole-v1\", pretrained_filepath=\"pretrained_model.h5\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 543
        },
        "id": "gb7eefXOsFD2",
        "outputId": "d2decd4d-4d05-42d7-c467-0cefa9f39a0d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pretrained model could not be loaded. Creating a new one.\n",
            "Episode 1/500, Total Reward: 19.0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (33, 5) + inhomogeneous part.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-a3d37a20cdde>\u001b[0m in \u001b[0;36m<cell line: 91>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;31m# Example usage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m     \u001b[0mtrained_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_rl_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"CartPole-v1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained_filepath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pretrained_model.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-3-a3d37a20cdde>\u001b[0m in \u001b[0;36mtrain_rl_model\u001b[0;34m(env_name, pretrained_filepath, episodes)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# Train the model using samples from the replay buffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m                 \u001b[0mminibatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_next\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mminibatch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                     \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mnumpy/random/mtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (33, 5) + inhomogeneous part."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# 1. Create a base model (pre-trained model)\n",
        "def create_base_model(input_dim, output_dim):\n",
        "    model = Sequential([\n",
        "        Dense(24, input_dim=input_dim, activation='relu'),\n",
        "        Dense(24, activation='relu'),\n",
        "        Dense(output_dim, activation='linear')  # Output layer for Q-values\n",
        "    ])\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
        "    return model\n",
        "\n",
        "# 2. Transfer learning: modify the model for the new task\n",
        "def transfer_model(base_model, new_output_dim):\n",
        "    base_model.pop()  # Remove the old output layer\n",
        "    base_model.add(Dense(new_output_dim, activation='linear'))  # Add new output layer\n",
        "    base_model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
        "    return base_model\n",
        "\n",
        "# 3. Define the RL environment\n",
        "env = gym.make('CartPole-v1')\n",
        "state_size = env.observation_space.shape[0]\n",
        "action_size = env.action_space.n\n",
        "\n",
        "# Assume base model was trained for a different environment\n",
        "# Create or load pre-trained base model\n",
        "pretrained_model = create_base_model(input_dim=state_size, output_dim=2)  # Example base model\n",
        "\n",
        "# Transfer the model for the new task\n",
        "model = transfer_model(pretrained_model, new_output_dim=action_size)\n",
        "\n",
        "# 4. Implement Q-Learning with the transferred model\n",
        "def train_dqn(env, model, episodes=1000, gamma=0.95, epsilon=1.0, epsilon_min=0.01, epsilon_decay=0.995):\n",
        "    for episode in range(episodes):\n",
        "        state = env.reset()\n",
        "        state = np.reshape(state, [1, state_size])\n",
        "        total_reward = 0\n",
        "\n",
        "        for time in range(500):  # Max time steps\n",
        "            # Choose action (epsilon-greedy)\n",
        "            if np.random.rand() <= epsilon:\n",
        "                action = np.random.choice(action_size)\n",
        "            else:\n",
        "                q_values = model.predict(state, verbose=0)\n",
        "                action = np.argmax(q_values[0])\n",
        "\n",
        "            # Take action\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            next_state = np.reshape(next_state, [1, state_size])\n",
        "\n",
        "            # Update Q-values\n",
        "            target = reward\n",
        "            if not done:\n",
        "                target += gamma * np.max(model.predict(next_state, verbose=0)[0])\n",
        "            q_values = model.predict(state, verbose=0)\n",
        "            q_values[0][action] = target\n",
        "\n",
        "            # Train the model\n",
        "            model.fit(state, q_values, verbose=0)\n",
        "\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "\n",
        "            if done:\n",
        "                print(f\"Episode {episode + 1}/{episodes}, Reward: {total_reward}\")\n",
        "                break\n",
        "\n",
        "        # Reduce exploration rate\n",
        "        if epsilon > epsilon_min:\n",
        "            epsilon *= epsilon_decay\n",
        "\n",
        "# 5. Train the model with transfer learning\n",
        "train_dqn(env, model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VgkLa1szseR0",
        "outputId": "e84c98e5-d4dc-4df1-c846-3487a5d9bae1"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1/1000, Reward: 16.0\n",
            "Episode 2/1000, Reward: 8.0\n",
            "Episode 3/1000, Reward: 16.0\n",
            "Episode 4/1000, Reward: 13.0\n",
            "Episode 5/1000, Reward: 29.0\n",
            "Episode 6/1000, Reward: 16.0\n",
            "Episode 7/1000, Reward: 14.0\n",
            "Episode 8/1000, Reward: 33.0\n",
            "Episode 9/1000, Reward: 29.0\n",
            "Episode 10/1000, Reward: 53.0\n",
            "Episode 11/1000, Reward: 15.0\n",
            "Episode 12/1000, Reward: 16.0\n",
            "Episode 13/1000, Reward: 18.0\n",
            "Episode 14/1000, Reward: 12.0\n",
            "Episode 15/1000, Reward: 16.0\n",
            "Episode 16/1000, Reward: 23.0\n",
            "Episode 17/1000, Reward: 26.0\n",
            "Episode 18/1000, Reward: 12.0\n",
            "Episode 19/1000, Reward: 27.0\n",
            "Episode 20/1000, Reward: 15.0\n",
            "Episode 21/1000, Reward: 16.0\n",
            "Episode 22/1000, Reward: 22.0\n",
            "Episode 23/1000, Reward: 11.0\n",
            "Episode 24/1000, Reward: 58.0\n",
            "Episode 25/1000, Reward: 16.0\n",
            "Episode 26/1000, Reward: 11.0\n",
            "Episode 27/1000, Reward: 34.0\n",
            "Episode 28/1000, Reward: 18.0\n",
            "Episode 29/1000, Reward: 26.0\n",
            "Episode 30/1000, Reward: 14.0\n",
            "Episode 31/1000, Reward: 26.0\n",
            "Episode 32/1000, Reward: 14.0\n",
            "Episode 33/1000, Reward: 22.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "dDQ2NDAXseA1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "from tensorflow.keras.models import Sequential, clone_model\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import random\n",
        "\n",
        "# 1. Define the base model (pre-trained model)\n",
        "def create_base_model(input_dim, output_dim):\n",
        "    model = Sequential([\n",
        "        Dense(128, input_dim=input_dim, activation='relu'),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dense(output_dim, activation='linear')  # Q-value output\n",
        "    ])\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
        "    return model\n",
        "\n",
        "# 2. Transfer learning: Adapt the model to the new task\n",
        "def transfer_model(base_model, new_output_dim):\n",
        "    base_model.pop()  # Remove the old output layer\n",
        "    base_model.add(Dense(new_output_dim, activation='linear'))  # Add new output layer\n",
        "    base_model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
        "    return base_model\n",
        "\n",
        "# 3. Define the Replay Buffer for experience storage\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, max_size):\n",
        "        self.buffer = deque(maxlen=max_size)\n",
        "\n",
        "    def add(self, experience):\n",
        "        self.buffer.append(experience)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.buffer, batch_size)\n",
        "\n",
        "    def size(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "# 4. Deep Q-Learning with Transfer Learning\n",
        "def train_dqn_with_transfer(env, base_model, episodes=500, gamma=0.99, epsilon=1.0, epsilon_min=0.1, epsilon_decay=0.995,\n",
        "                             batch_size=64, buffer_size=100000, update_target_freq=10):\n",
        "    state_size = env.observation_space.shape[0]\n",
        "    action_size = env.action_space.n\n",
        "\n",
        "    # Transfer model\n",
        "    model = transfer_model(base_model, new_output_dim=action_size)\n",
        "    target_model = clone_model(model)  # Target network\n",
        "    target_model.set_weights(model.get_weights())  # Synchronize weights\n",
        "\n",
        "    replay_buffer = ReplayBuffer(buffer_size)\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        state = env.reset()\n",
        "        state = np.reshape(state, [1, state_size])\n",
        "        total_reward = 0\n",
        "\n",
        "        for step in range(500):\n",
        "            # Choose action using epsilon-greedy policy\n",
        "            if np.random.rand() <= epsilon:\n",
        "                action = np.random.choice(action_size)\n",
        "            else:\n",
        "                q_values = model.predict(state, verbose=0)\n",
        "                action = np.argmax(q_values[0])\n",
        "\n",
        "            # Perform the action\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            next_state = np.reshape(next_state, [1, state_size])\n",
        "\n",
        "            # Store experience in replay buffer\n",
        "            replay_buffer.add((state, action, reward, next_state, done))\n",
        "\n",
        "            # Update state\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "\n",
        "            # Training step\n",
        "            if replay_buffer.size() >= batch_size:\n",
        "                minibatch = replay_buffer.sample(batch_size)\n",
        "                for s, a, r, s_next, d in minibatch:\n",
        "                    target = model.predict(s, verbose=0)\n",
        "                    if d:\n",
        "                        target[0][a] = r\n",
        "                    else:\n",
        "                        t = target_model.predict(s_next, verbose=0)\n",
        "                        target[0][a] = r + gamma * np.amax(t[0])\n",
        "\n",
        "                    model.fit(s, target, verbose=0, batch_size=1)\n",
        "\n",
        "            # Update target network periodically\n",
        "            if step % update_target_freq == 0:\n",
        "                target_model.set_weights(model.get_weights())\n",
        "\n",
        "            if done:\n",
        "                print(f\"Episode {episode + 1}/{episodes}, Reward: {total_reward}, Epsilon: {epsilon:.2f}\")\n",
        "                break\n",
        "\n",
        "        # Reduce exploration rate\n",
        "        if epsilon > epsilon_min:\n",
        "            epsilon *= epsilon_decay\n",
        "\n",
        "    return model\n",
        "\n",
        "# 5. Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    env = gym.make('CartPole-v1')\n",
        "    state_size = env.observation_space.shape[0]\n",
        "    action_size = env.action_space.n\n",
        "\n",
        "    # Create or load a pre-trained base model\n",
        "    base_model = create_base_model(input_dim=state_size, output_dim=2)  # Example pre-trained model\n",
        "\n",
        "    # Train the agent in the new environment with transfer learning\n",
        "    trained_model = train_dqn_with_transfer(env, base_model)\n"
      ],
      "metadata": {
        "id": "QIz1CXPXs3Gc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}